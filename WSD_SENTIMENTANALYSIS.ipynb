{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aG8kKO5oQzyq",
    "outputId": "017715b4-e1b8-433d-ae7f-d726d98c9387"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "गोवा की यात्रा बहुत अच्छी रही। समुद्र तट बहुत गर्म थे। मुझे समुद्र तट पर खेलने में बहुत मजा आया। मेरी बेटी बहुत गुस् से में थी। जिंदगी में कई बार ऐसा होता है की जब हम अपने सोचने का तरीका बदलते है तब हमारी जिंदगी भी बदल जाती है । जिंदगी में जो कुछ भी होता है वो सिर्फ ही होता है बाकि तो हम जो हुआ है उसके बारे में क्या सोचते है यानि की हमारा reaction ही होता है । एक लड़का था जो देख नहीं सकता था । ये लड़का भीख मांगकर अपना जीवन गुजारता था । एक दिन ये अँधा लड़का हर रोज की तरह एक बड़ी सी building के आगे बैठकर भीख मांग रहा था । उसी समय वहा से एक अनमोल नामका लड़का गुजरता है और इस अंधे लड़के को भीख मांगता हुआ देखता है । अनमोल उसी building में एक ऑफिस में काम करता था । जब अनमोल ने देखा की ये लड़का भीख मांग रहा है तो वो उसकी मदद करने के लिए उसके पास पहुँचता है । अनमोल वहा जाकर देखता है की इस लड़के के पास एक डिब्बा है और उस डिब्बे में थोड़े सिक्के भी है । कुछ लोग आते है इस लड़के को पैसे देते है और कुछ लोग उसे बस देखकर ही चले जाते है । अनमोल ने ये भी देखा की इस लड़के के पीछे एक बोर्ड लगा हुआ था जिस पर लिखा था की में एक अँधा लड़का हु मेरी मदद कीजिये । ये सब देखकर अनमोल अब ये सोचने लगा की इस दुनिया में भी बड़े कमाल के लोग है जिसे मदद की जरुरत है उसे कोई नहीं करता है । उस बोर्ड पर जो लिखा था उसे वो मिटा देता है और उसकी जगह पर और कुछ लिख देता है । अब अनमोल कुछ पैसे उस डिब्बे में रखता है और ऑफिस की और बढ़ता है । वो कहता है की भैया मेने आपको आपकी आने की आहट से ही पहचान लिया । आप वही है जो सुबह आये थे और मेरे इस बोर्ड पर कुछ लिख कर गए थे । में आपको सिर्फ इतना पूछना चाहता हु की आपने ऐसा क्या लिखा था मेरे बोर्ड पर की जो भी यहाँ से गुजरता है वो मुझे कुछ ना कुछ देकर ही जाता है । अनमोल ने कहा की मेने जो तुम्हारे बोर्ड पर लिखा हुआ था उसे मिटा दिया । तुम्हारे बोर्ड पर लिखा हुआ था की में एक अँधा लड़का हु मेरी मदद कीजिये मेने उसको मिटा दिया और ये लिख दिया की आज का दिन बहुत खूबसूरत है लेकिन में देख नहीं सकता हु\n",
      "Lemmatized Tokens:\n",
      "[['गोवा', 'की', 'यात्रा', 'बहुत', 'अच्छी', 'रही।', 'समुद्र', 'तट', 'बहुत', 'गर्म', 'थे।', 'मुझे', 'समुद्र', 'तट', 'पर', 'खेलने', 'में', 'बहुत', 'मजा', 'आया।', 'मेरी', 'बेटी', 'बहुत', 'गुस्', 'से', 'में', 'थी।', 'जिंदगी', 'में', 'कई', 'बार', 'ऐसा', 'होता', 'है', 'की', 'जब', 'हम', 'अपने', 'सोचने', 'का', 'तरीका', 'बदलते', 'है', 'तब', 'हमारी', 'जिंदगी', 'भी', 'बदल', 'जाती', 'है', '।', 'जिंदगी', 'में', 'जो', 'कुछ', 'भी', 'होता', 'है', 'वो', 'सिर्फ', 'ही', 'होता', 'है', 'बाकि', 'तो', 'हम', 'जो', 'हुआ', 'है', 'उसके', 'बारे', 'में', 'क्या', 'सोचते', 'है', 'यानि', 'की', 'हमारा', 'reaction', 'ही', 'होता', 'है', '।', 'एक', 'लड़का', 'था', 'जो', 'देख', 'नहीं', 'सकता', 'था', '।', 'ये', 'लड़का', 'भीख', 'मांगकर', 'अपना', 'जीवन', 'गुजारता', 'था', '।', 'एक', 'दिन', 'ये', 'अँधा', 'लड़का', 'हर', 'रोज', 'की', 'तरह', 'एक', 'बड़ी', 'सी', 'building', 'के', 'आगे', 'बैठकर', 'भीख', 'मांग', 'रहा', 'था', '।', 'उसी', 'समय', 'वहा', 'से', 'एक', 'अनमोल', 'नामका', 'लड़का', 'गुजरता', 'है', 'और', 'इस', 'अंधे', 'लड़के', 'को', 'भीख', 'मांगता', 'हुआ', 'देखता', 'है', '।', 'अनमोल', 'उसी', 'building', 'में', 'एक', 'ऑफिस', 'में', 'काम', 'करता', 'था', '।', 'जब', 'अनमोल', 'ने', 'देखा', 'की', 'ये', 'लड़का', 'भीख', 'मांग', 'रहा', 'है', 'तो', 'वो', 'उसकी', 'मदद', 'करने', 'के', 'लिए', 'उसके', 'पास', 'पहुँचता', 'है', '।', 'अनमोल', 'वहा', 'जाकर', 'देखता', 'है', 'की', 'इस', 'लड़के', 'के', 'पास', 'एक', 'डिब्बा', 'है', 'और', 'उस', 'डिब्बे', 'में', 'थोड़े', 'सिक्के', 'भी', 'है', '।', 'कुछ', 'लोग', 'आते', 'है', 'इस', 'लड़के', 'को', 'पैसे', 'देते', 'है', 'और', 'कुछ', 'लोग', 'उसे', 'बस', 'देखकर', 'ही', 'चले', 'जाते', 'है', '।', 'अनमोल', 'ने', 'ये', 'भी', 'देखा', 'की', 'इस', 'लड़के', 'के', 'पीछे', 'एक', 'बोर्ड', 'लगा', 'हुआ', 'था', 'जिस', 'पर', 'लिखा', 'था', 'की', 'में', 'एक', 'अँधा', 'लड़का', 'हु', 'मेरी', 'मदद', 'कीजिये', '।', 'ये', 'सब', 'देखकर', 'अनमोल', 'अब', 'ये', 'सोचने', 'लगा', 'की', 'इस', 'दुनिया', 'में', 'भी', 'बड़े', 'कमाल', 'के', 'लोग', 'है', 'जिसे', 'मदद', 'की', 'जरुरत', 'है', 'उसे', 'कोई', 'नहीं', 'करता', 'है', '।', 'उस', 'बोर्ड', 'पर', 'जो', 'लिखा', 'था', 'उसे', 'वो', 'मिटा', 'देता', 'है', 'और', 'उसकी', 'जगह', 'पर', 'और', 'कुछ', 'लिख', 'देता', 'है', '।', 'अब', 'अनमोल', 'कुछ', 'पैसे', 'उस', 'डिब्बे', 'में', 'रखता', 'है', 'और', 'ऑफिस', 'की', 'और', 'बढ़ता', 'है', '।', 'वो', 'कहता', 'है', 'की', 'भैया', 'मेने', 'आपको', 'आपकी', 'आने', 'की', 'आहट', 'से', 'ही', 'पहचान', 'लिया', '।', 'आप', 'वही', 'है', 'जो', 'सुबह', 'आये', 'थे', 'और', 'मेरे', 'इस', 'बोर्ड', 'पर', 'कुछ', 'लिख', 'कर', 'गए', 'थे', '।', 'में', 'आपको', 'सिर्फ', 'इतना', 'पूछना', 'चाहता', 'हु', 'की', 'आपने', 'ऐसा', 'क्या', 'लिखा', 'था', 'मेरे', 'बोर्ड', 'पर', 'की', 'जो', 'भी', 'यहाँ', 'से', 'गुजरता', 'है', 'वो', 'मुझे', 'कुछ', 'ना', 'कुछ', 'देकर', 'ही', 'जाता', 'है', '।', 'अनमोल', 'ने', 'कहा', 'की', 'मेने', 'जो', 'तुम्हारे', 'बोर्ड', 'पर', 'लिखा', 'हुआ', 'था', 'उसे', 'मिटा', 'दिया', '।', 'तुम्हारे', 'बोर्ड', 'पर', 'लिखा', 'हुआ', 'था', 'की', 'में', 'एक', 'अँधा', 'लड़का', 'हु', 'मेरी', 'मदद', 'कीजिये', 'मेने', 'उसको', 'मिटा', 'दिया', 'और', 'ये', 'लिख', 'दिया', 'की', 'आज', 'का', 'दिन', 'बहुत', 'खूबसूरत', 'है', 'लेकिन', 'में', 'देख', 'नहीं', 'सकता', 'हु']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove extra whitespace\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    # Remove punctuations\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "\n",
    "    # Remove digits\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove stopwords\n",
    "    #stop_words = set(stopwords.words('hindi'))\n",
    "    stopwords_file = open(\"/content/stopwords.txt\", encoding=\"utf-8\")\n",
    "    stopwords = stopwords_file.read().split()\n",
    "    text_tokens = text.split()\n",
    "    text_tokens = [token for token in text_tokens if token not in stopwords]\n",
    "    text = \" \".join(text_tokens)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "with open(\"/content/SampleHindiText.txt\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "preprocessed_text = preprocess_text(text)\n",
    "print(preprocessed_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# set up the stemmer and lemmatizer\n",
    "#stemmer = SnowballStemmer(\"hindi\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# tokenize the text into sentences and then words\n",
    "sentences = preprocessed_text.split('\\n')\n",
    "tokens = [word_tokenize(s) for s in sentences]\n",
    "\n",
    "# perform stemming and lemmatization\n",
    "stemmed_tokens = []\n",
    "lemmatized_tokens = []\n",
    "for sentence in tokens:\n",
    "    stemmed_sentence = []\n",
    "    lemmatized_sentence = []\n",
    "    for word in sentence:\n",
    "        #stemmed_word = stemmer.stem(word)\n",
    "        lemmatized_word = lemmatizer.lemmatize(word)\n",
    "        #stemmed_sentence.append(stemmed_word)\n",
    "        lemmatized_sentence.append(lemmatized_word)\n",
    "    #stemmed_tokens.append(stemmed_sentence)\n",
    "    lemmatized_tokens.append(lemmatized_sentence)\n",
    "\n",
    "# print out the results\n",
    "#print(\"Stemmed Tokens:\")\n",
    "#print(stemmed_tokens)\n",
    "print(\"Lemmatized Tokens:\")\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZgIvulcUQz-P",
    "outputId": "77992a3b-5038-4954-9f67-69b8295b7959"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens:\n",
      "[['गोव', 'क', 'यात्र', 'बहुत', 'अच्छ', 'रह', 'समुद्र', 'तट', 'बहुत', 'गर्म', 'थ', 'मुझ', 'समुद्र', 'तट', 'पर', 'खेलन', 'मे', 'बहुत', 'मज', 'आय', 'मेर', 'बेट', 'बहुत', 'गुस्', 'स', 'मे', 'थ', 'जिंदग', 'मे', 'कई', 'बार', 'ऐस', 'होत', 'ह', 'क', 'जब', 'हम', 'अपन', 'सोचन', 'क', 'तरीक', 'बदलत', 'ह', 'तब', 'हमार', 'जिंदग', 'भ', 'बदल', 'जात', 'ह', '।', 'जिंदग', 'मे', 'ज', 'कुछ', 'भ', 'होत', 'ह', 'व', 'सिर्फ', 'ह', 'होत', 'ह', 'बाक', 'त', 'हम', 'ज', 'हुआ', 'ह', 'उसक', 'बार', 'मे', 'क्य', 'सोचत', 'ह', 'यान', 'क', 'हमार', 'reaction', 'ह', 'होत', 'ह', '।', 'एक', 'लड़क', 'थ', 'ज', 'देख', 'नही', 'सकत', 'थ', '।', 'य', 'लड़क', 'भीख', 'मांगकर', 'अपन', 'जीवन', 'गुजारत', 'थ', '।', 'एक', 'दिन', 'य', 'अँध', 'लड़क', 'हर', 'रोज', 'क', 'तरह', 'एक', 'बड़', 'स', 'building', 'क', 'आग', 'बैठकर', 'भीख', 'मांग', 'रह', 'थ', '।', 'उस', 'समय', 'वह', 'स', 'एक', 'अनमोल', 'नामक', 'लड़क', 'गुजरत', 'ह', 'और', 'इस', 'अंध', 'लड़क', 'क', 'भीख', 'मांगत', 'हुआ', 'देखत', 'ह', '।', 'अनमोल', 'उस', 'building', 'मे', 'एक', 'ऑफिस', 'मे', 'काम', 'करत', 'थ', '।', 'जब', 'अनमोल', 'न', 'देख', 'क', 'य', 'लड़क', 'भीख', 'मांग', 'रह', 'ह', 'त', 'व', 'उसक', 'मदद', 'करन', 'क', 'लिए', 'उसक', 'पास', 'पहुँचत', 'ह', '।', 'अनमोल', 'वह', 'जाकर', 'देखत', 'ह', 'क', 'इस', 'लड़क', 'क', 'पास', 'एक', 'डिब्ब', 'ह', 'और', 'उस', 'डिब्ब', 'मे', 'थोड़', 'सिक्क', 'भ', 'ह', '।', 'कुछ', 'लोग', 'आत', 'ह', 'इस', 'लड़क', 'क', 'पैस', 'देत', 'ह', 'और', 'कुछ', 'लोग', 'उस', 'बस', 'देखकर', 'ह', 'चल', 'जात', 'ह', '।', 'अनमोल', 'न', 'य', 'भ', 'देख', 'क', 'इस', 'लड़क', 'क', 'पीछ', 'एक', 'बोर्ड', 'लग', 'हुआ', 'थ', 'जिस', 'पर', 'लिख', 'थ', 'क', 'मे', 'एक', 'अँध', 'लड़क', 'ह', 'मेर', 'मदद', 'कीजिय', '।', 'य', 'सब', 'देखकर', 'अनमोल', 'अब', 'य', 'सोचन', 'लग', 'क', 'इस', 'दुनिय', 'मे', 'भ', 'बड़', 'कमाल', 'क', 'लोग', 'ह', 'जिस', 'मदद', 'क', 'जरुरत', 'ह', 'उस', 'कोई', 'नही', 'करत', 'ह', '।', 'उस', 'बोर्ड', 'पर', 'ज', 'लिख', 'थ', 'उस', 'व', 'मिट', 'देत', 'ह', 'और', 'उसक', 'जगह', 'पर', 'और', 'कुछ', 'लिख', 'देत', 'ह', '।', 'अब', 'अनमोल', 'कुछ', 'पैस', 'उस', 'डिब्ब', 'मे', 'रखत', 'ह', 'और', 'ऑफिस', 'क', 'और', 'बढ़त', 'ह', '।', 'व', 'कहत', 'ह', 'क', 'भैय', 'मेन', 'आपक', 'आपक', 'आन', 'क', 'आहट', 'स', 'ह', 'पहचान', 'लिय', '।', 'आप', 'वह', 'ह', 'ज', 'सुबह', 'आय', 'थ', 'और', 'मेर', 'इस', 'बोर्ड', 'पर', 'कुछ', 'लिख', 'कर', 'गए', 'थ', '।', 'मे', 'आपक', 'सिर्फ', 'इतन', 'पूछन', 'चाहत', 'ह', 'क', 'आपन', 'ऐस', 'क्य', 'लिख', 'थ', 'मेर', 'बोर्ड', 'पर', 'क', 'ज', 'भ', 'यहा', 'स', 'गुजरत', 'ह', 'व', 'मुझ', 'कुछ', 'न', 'कुछ', 'देकर', 'ह', 'जात', 'ह', '।', 'अनमोल', 'न', 'कह', 'क', 'मेन', 'ज', 'तुम्हार', 'बोर्ड', 'पर', 'लिख', 'हुआ', 'थ', 'उस', 'मिट', 'दिय', '।', 'तुम्हार', 'बोर्ड', 'पर', 'लिख', 'हुआ', 'थ', 'क', 'मे', 'एक', 'अँध', 'लड़क', 'ह', 'मेर', 'मदद', 'कीजिय', 'मेन', 'उसक', 'मिट', 'दिय', 'और', 'य', 'लिख', 'दिय', 'क', 'आज', 'क', 'दिन', 'बहुत', 'खूबसूरत', 'ह', 'लेकिन', 'मे', 'देख', 'नही', 'सकत', 'ह']]\n"
     ]
    }
   ],
   "source": [
    "# define the suffixes to remove from Hindi words\n",
    "suffixes = [\"ा\", \"ि\", \"ी\", \"ु\", \"ू\", \"े\", \"ै\", \"ो\", \"ौ\", \"ँ\", \"ं\", \"ः\", \"े।\", \"ो।\", \"ू।\", \"ा।\", \"ी।\"]\n",
    "\n",
    "\n",
    "# perform stemming\n",
    "stemmed_tokens = []\n",
    "for sentence in tokens:\n",
    "    stemmed_sentence = []\n",
    "    for word in sentence:\n",
    "        for suffix in suffixes:\n",
    "            if word.endswith(suffix):\n",
    "                word = word[:-len(suffix)]\n",
    "                break\n",
    "        stemmed_sentence.append(word)\n",
    "    stemmed_tokens.append(stemmed_sentence)\n",
    "\n",
    "# print out the results\n",
    "print(\"Stemmed Tokens:\")\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVG6RDDqcgf_",
    "outputId": "2c03afe7-550e-428b-beef-1cf77169f6ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['गोव', 'क', 'यात्र', 'बहुत', 'अच्छ', 'रह', 'समुद्र', 'तट', 'बहुत', 'गर्म', 'थ', 'मुझ', 'समुद्र', 'तट', 'पर', 'खेलन', 'मे', 'बहुत', 'मज', 'आय', 'मेर', 'बेट', 'बहुत', 'गुस्', 'स', 'मे', 'थ', 'जिंदग', 'मे', 'कई', 'बार', 'ऐस', 'होत', 'ह', 'क', 'जब', 'हम', 'अपन', 'सोचन', 'क', 'तरीक', 'बदलत', 'ह', 'तब', 'हमार', 'जिंदग', 'भ', 'बदल', 'जात', 'ह', '।', 'जिंदग', 'मे', 'ज', 'कुछ', 'भ', 'होत', 'ह', 'व', 'सिर्फ', 'ह', 'होत', 'ह', 'बाक', 'त', 'हम', 'ज', 'हुआ', 'ह', 'उसक', 'बार', 'मे', 'क्य', 'सोचत', 'ह', 'यान', 'क', 'हमार', 'reaction', 'ह', 'होत', 'ह', '।', 'एक', 'लड़क', 'थ', 'ज', 'देख', 'नही', 'सकत', 'थ', '।', 'य', 'लड़क', 'भीख', 'मांगकर', 'अपन', 'जीवन', 'गुजारत', 'थ', '।', 'एक', 'दिन', 'य', 'अँध', 'लड़क', 'हर', 'रोज', 'क', 'तरह', 'एक', 'बड़', 'स', 'building', 'क', 'आग', 'बैठकर', 'भीख', 'मांग', 'रह', 'थ', '।', 'उस', 'समय', 'वह', 'स', 'एक', 'अनमोल', 'नामक', 'लड़क', 'गुजरत', 'ह', 'और', 'इस', 'अंध', 'लड़क', 'क', 'भीख', 'मांगत', 'हुआ', 'देखत', 'ह', '।', 'अनमोल', 'उस', 'building', 'मे', 'एक', 'ऑफिस', 'मे', 'काम', 'करत', 'थ', '।', 'जब', 'अनमोल', 'न', 'देख', 'क', 'य', 'लड़क', 'भीख', 'मांग', 'रह', 'ह', 'त', 'व', 'उसक', 'मदद', 'करन', 'क', 'लिए', 'उसक', 'पास', 'पहुँचत', 'ह', '।', 'अनमोल', 'वह', 'जाकर', 'देखत', 'ह', 'क', 'इस', 'लड़क', 'क', 'पास', 'एक', 'डिब्ब', 'ह', 'और', 'उस', 'डिब्ब', 'मे', 'थोड़', 'सिक्क', 'भ', 'ह', '।', 'कुछ', 'लोग', 'आत', 'ह', 'इस', 'लड़क', 'क', 'पैस', 'देत', 'ह', 'और', 'कुछ', 'लोग', 'उस', 'बस', 'देखकर', 'ह', 'चल', 'जात', 'ह', '।', 'अनमोल', 'न', 'य', 'भ', 'देख', 'क', 'इस', 'लड़क', 'क', 'पीछ', 'एक', 'बोर्ड', 'लग', 'हुआ', 'थ', 'जिस', 'पर', 'लिख', 'थ', 'क', 'मे', 'एक', 'अँध', 'लड़क', 'ह', 'मेर', 'मदद', 'कीजिय', '।', 'य', 'सब', 'देखकर', 'अनमोल', 'अब', 'य', 'सोचन', 'लग', 'क', 'इस', 'दुनिय', 'मे', 'भ', 'बड़', 'कमाल', 'क', 'लोग', 'ह', 'जिस', 'मदद', 'क', 'जरुरत', 'ह', 'उस', 'कोई', 'नही', 'करत', 'ह', '।', 'उस', 'बोर्ड', 'पर', 'ज', 'लिख', 'थ', 'उस', 'व', 'मिट', 'देत', 'ह', 'और', 'उसक', 'जगह', 'पर', 'और', 'कुछ', 'लिख', 'देत', 'ह', '।', 'अब', 'अनमोल', 'कुछ', 'पैस', 'उस', 'डिब्ब', 'मे', 'रखत', 'ह', 'और', 'ऑफिस', 'क', 'और', 'बढ़त', 'ह', '।', 'व', 'कहत', 'ह', 'क', 'भैय', 'मेन', 'आपक', 'आपक', 'आन', 'क', 'आहट', 'स', 'ह', 'पहचान', 'लिय', '।', 'आप', 'वह', 'ह', 'ज', 'सुबह', 'आय', 'थ', 'और', 'मेर', 'इस', 'बोर्ड', 'पर', 'कुछ', 'लिख', 'कर', 'गए', 'थ', '।', 'मे', 'आपक', 'सिर्फ', 'इतन', 'पूछन', 'चाहत', 'ह', 'क', 'आपन', 'ऐस', 'क्य', 'लिख', 'थ', 'मेर', 'बोर्ड', 'पर', 'क', 'ज', 'भ', 'यहा', 'स', 'गुजरत', 'ह', 'व', 'मुझ', 'कुछ', 'न', 'कुछ', 'देकर', 'ह', 'जात', 'ह', '।', 'अनमोल', 'न', 'कह', 'क', 'मेन', 'ज', 'तुम्हार', 'बोर्ड', 'पर', 'लिख', 'हुआ', 'थ', 'उस', 'मिट', 'दिय', '।', 'तुम्हार', 'बोर्ड', 'पर', 'लिख', 'हुआ', 'थ', 'क', 'मे', 'एक', 'अँध', 'लड़क', 'ह', 'मेर', 'मदद', 'कीजिय', 'मेन', 'उसक', 'मिट', 'दिय', 'और', 'य', 'लिख', 'दिय', 'क', 'आज', 'क', 'दिन', 'बहुत', 'खूबसूरत', 'ह', 'लेकिन', 'मे', 'देख', 'नही', 'सकत', 'ह']]\n"
     ]
    }
   ],
   "source": [
    "k='|'\n",
    "while(k in stemmed_tokens[0]):\n",
    "  stemmed_tokens[0].remove(k)\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8DQfN8ybT4ZD",
    "outputId": "dc0a0d2c-aaff-4ec1-d55e-21783f70f38e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting hindiwsd\n",
      "  Downloading hindiwsd-1.1.2-py3-none-any.whl (79.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nltk==3.4.5\n",
      "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting bleach==4.1.0\n",
      "  Downloading bleach-4.1.0-py2.py3-none-any.whl (157 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.9/157.9 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl==3.0.0\n",
      "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.9/dist-packages (from hindiwsd) (2.8.2)\n",
      "Collecting keras==2.6.0\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard==2.7.0\n",
      "  Downloading tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting Werkzeug==2.0.2\n",
      "  Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.9/288.9 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting absl-py==0.15.0\n",
      "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.9/dist-packages (from hindiwsd) (0.4.8)\n",
      "Collecting certifi==2021.10.8\n",
      "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.2/149.2 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting Markdown==3.3.4\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas==1.3.4\n",
      "  Downloading pandas-1.3.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h5py==3.1.0\n",
      "  Downloading h5py-3.1.0-cp39-cp39-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.9/dist-packages (from hindiwsd) (1.6.3)\n",
      "Collecting selenium==3.141.0\n",
      "  Downloading selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m904.6/904.6 kB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rfc3986==1.5.0\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting wrapt==1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting termcolor==1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting typer==0.3.2\n",
      "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
      "Collecting packaging==21.0\n",
      "  Downloading packaging-21.0-py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server==0.6.1\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy==1.7.1\n",
      "  Downloading scipy-1.7.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.5/28.5 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex==2020.10.11\n",
      "  Downloading regex-2020.10.11-cp39-cp39-manylinux2010_x86_64.whl (668 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m668.8/668.8 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting importlib-metadata==4.8.1\n",
      "  Downloading importlib_metadata-4.8.1-py3-none-any.whl (17 kB)\n",
      "Collecting typing-extensions==3.7.4.3\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting requests-oauthlib==1.3.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pep517==0.12.0\n",
      "  Downloading pep517-0.12.0-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: toml==0.10.2 in /usr/local/lib/python3.9/dist-packages (from hindiwsd) (0.10.2)\n",
      "Collecting google-auth==2.3.0\n",
      "  Downloading google_auth-2.3.0-py2.py3-none-any.whl (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.1/154.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting build==0.7.0\n",
      "  Downloading build-0.7.0-py3-none-any.whl (16 kB)\n",
      "Collecting twine==3.4.2\n",
      "  Downloading twine-3.4.2-py3-none-any.whl (34 kB)\n",
      "Collecting tensorflow-estimator==2.6.0\n",
      "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.9/462.9 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting seqeval==1.2.2\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting click==7.1.2\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tomli==1.2.1\n",
      "  Downloading tomli-1.2.1-py3-none-any.whl (11 kB)\n",
      "Collecting colorama==0.4.4\n",
      "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting Keras-Preprocessing==1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytz==2021.3\n",
      "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.5/503.5 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib==1.1.0\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.0/307.0 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow==2.6.0\n",
      "  Downloading tensorflow-2.6.0-cp39-cp39-manylinux2010_x86_64.whl (458.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.4/458.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting readme-renderer==30.0\n",
      "  Downloading readme_renderer-30.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting pkginfo==1.7.1\n",
      "  Downloading pkginfo-1.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting requests-toolbelt==0.9.1\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit==1.8.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.2/781.2 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer==2.0.7\n",
      "  Downloading charset_normalizer-2.0.7-py3-none-any.whl (38 kB)\n",
      "Collecting backports.functools-lru-cache==1.6.4\n",
      "  Downloading backports.functools_lru_cache-1.6.4-py2.py3-none-any.whl (5.9 kB)\n",
      "Collecting protobuf==3.19.0\n",
      "  Downloading protobuf-3.19.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting polyglot-tokenizer==2.0.2\n",
      "  Downloading polyglot_tokenizer-2.0.2-py2.py3-none-any.whl (323 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.3/323.3 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting spello==1.2.0\n",
      "  Downloading spello-1.2.0-py3-none-any.whl (35 kB)\n",
      "Collecting indic-transliteration==2.3.2\n",
      "  Downloading indic_transliteration-2.3.2-py3-none-any.whl (119 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting clang==5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting idna==3.3\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pywin32-ctypes==0.2.0\n",
      "  Downloading pywin32_ctypes-0.2.0-py2.py3-none-any.whl (28 kB)\n",
      "Collecting oauthlib==3.1.1\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.2/146.2 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cachetools==4.2.4\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting requests==2.26.0\n",
      "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pycodestyle==2.8.0\n",
      "  Downloading pycodestyle-2.8.0-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docutils==0.17.1\n",
      "  Downloading docutils-0.17.1-py2.py3-none-any.whl (575 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.5/575.5 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyparsing==3.0.1\n",
      "  Downloading pyparsing-3.0.1-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.1/96.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib==0.4.6\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting Pygments==2.10.0\n",
      "  Downloading Pygments-2.10.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting zipp==3.6.0\n",
      "  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting python-crfsuite==0.9.7\n",
      "  Downloading python-crfsuite-0.9.7.tar.gz (432 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.2/432.2 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting urllib3==1.26.7\n",
      "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting keyring==23.2.1\n",
      "  Downloading keyring-23.2.1-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.9/dist-packages (from hindiwsd) (0.2.0)\n",
      "Collecting twisted==21.7.0\n",
      "  Downloading Twisted-21.7.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.9/dist-packages (from hindiwsd) (3.3.0)\n",
      "Collecting numpy==1.19.5\n",
      "  Downloading numpy-1.19.5-cp39-cp39-manylinux2010_x86_64.whl (14.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn==1.0\n",
      "  Downloading scikit_learn-1.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.9/dist-packages (from hindiwsd) (0.4.0)\n",
      "Collecting tqdm==4.62.3\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pbr==2.1.0\n",
      "  Downloading pbr-2.1.0-py2.py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rsa==4.7.2\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting pyiwn==0.0.5\n",
      "  Downloading pyiwn-0.0.5-py3-none-any.whl (12 kB)\n",
      "Collecting six==1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.9/dist-packages (from hindiwsd) (0.2.8)\n",
      "Collecting autopep8==1.6.0\n",
      "  Downloading autopep8-1.6.0-py2.py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers==1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: webencodings==0.5.1 in /usr/local/lib/python3.9/dist-packages (from hindiwsd) (0.5.1)\n",
      "Collecting grpcio==1.41.0\n",
      "  Downloading grpcio-1.41.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse==1.6.3->hindiwsd) (0.40.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.9/dist-packages (from google-auth==2.3.0->hindiwsd) (67.6.1)\n",
      "Collecting SecretStorage>=3.2\n",
      "  Downloading SecretStorage-3.3.3-py3-none-any.whl (15 kB)\n",
      "Collecting jeepney>=0.4.2\n",
      "  Downloading jeepney-0.8.0-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting constantly>=15.1\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting incremental>=21.3.0\n",
      "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.9/dist-packages (from twisted==21.7.0->hindiwsd) (22.2.0)\n",
      "Collecting zope.interface>=4.4.2\n",
      "  Downloading zope.interface-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (246 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.1/246.1 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting Automat>=0.8.0\n",
      "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.9/dist-packages (from SecretStorage>=3.2->keyring==23.2.1->hindiwsd) (40.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.9/dist-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring==23.2.1->hindiwsd) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring==23.2.1->hindiwsd) (2.21)\n",
      "Building wheels for collected packages: clang, nltk, python-crfsuite, seqeval, termcolor, wrapt\n",
      "  Building wheel for clang (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30692 sha256=973f5f0fcbfc2ca7178733bb97dbca859632542682928ca81f961167bda0d770\n",
      "  Stored in directory: /root/.cache/pip/wheels/3a/ce/7a/27094f689461801c934296d07078773603663dfcaca63bb064\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449921 sha256=7f7e96f00d54232de8a802bb3568c4e0f908c32fc2094c9c55d150fd8e957528\n",
      "  Stored in directory: /root/.cache/pip/wheels/04/32/57/69e42ad50941013def31e288c6e06bb569442dd993a123cb76\n",
      "  Building wheel for python-crfsuite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for python-crfsuite: filename=python_crfsuite-0.9.7-cp39-cp39-linux_x86_64.whl size=984480 sha256=c77f4a8ee2742b2cae554ffcac1633125f12c8639ea59280126e59da6d9a0999\n",
      "  Stored in directory: /root/.cache/pip/wheels/07/d8/b4/63d7ef5d4c619b8f1fea39c5c5b7b52dbf014628134c144d22\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=2d308657accb52a84a57c3c35e4c78744e847791b32b348eb9f7c146523300a7\n",
      "  Stored in directory: /root/.cache/pip/wheels/e2/a5/92/2c80d1928733611c2747a9820e1324a6835524d9411510c142\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4845 sha256=6cd3e42fc91ce18c70441ae98f2e131606afd12cea5f0bdee8bec721ca70e399\n",
      "  Stored in directory: /root/.cache/pip/wheels/b6/0d/90/0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp39-cp39-linux_x86_64.whl size=75944 sha256=b1163b40d2a0d25c1ceeb909c1b929ea1e602926859351b8e753bdfd23b85094\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/23/68/efe259aaca055e93b08e74fbe512819c69a2155c11ba3c0f10\n",
      "Successfully built clang nltk python-crfsuite seqeval termcolor wrapt\n",
      "Installing collected packages: wrapt, typing-extensions, termcolor, tensorflow-estimator, tensorboard-plugin-wit, rfc3986, regex, pywin32-ctypes, pytz, python-crfsuite, pkginfo, pbr, keras, incremental, flatbuffers, constantly, clang, certifi, zope.interface, zipp, Werkzeug, urllib3, tqdm, tomli, threadpoolctl, tensorboard-data-server, six, rsa, pyparsing, Pygments, pycodestyle, protobuf, oauthlib, numpy, Markdown, joblib, jeepney, idna, docutils, colorama, click, charset-normalizer, cachetools, backports.functools-lru-cache, typer, selenium, scipy, requests, polyglot-tokenizer, pep517, packaging, nltk, Keras-Preprocessing, importlib-metadata, hyperlink, h5py, grpcio, google-auth, autopep8, Automat, absl-py, twisted, spello, SecretStorage, scikit-learn, requests-toolbelt, requests-oauthlib, pandas, indic-transliteration, build, bleach, seqeval, readme-renderer, pyiwn, keyring, google-auth-oauthlib, twine, tensorboard, tensorflow, hindiwsd\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.14.1\n",
      "    Uninstalling wrapt-1.14.1:\n",
      "      Successfully uninstalled wrapt-1.14.1\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "  Attempting uninstall: termcolor\n",
      "    Found existing installation: termcolor 2.2.0\n",
      "    Uninstalling termcolor-2.2.0:\n",
      "      Successfully uninstalled termcolor-2.2.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.12.0\n",
      "    Uninstalling tensorflow-estimator-2.12.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
      "  Attempting uninstall: tensorboard-plugin-wit\n",
      "    Found existing installation: tensorboard-plugin-wit 1.8.1\n",
      "    Uninstalling tensorboard-plugin-wit-1.8.1:\n",
      "      Successfully uninstalled tensorboard-plugin-wit-1.8.1\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2022.10.31\n",
      "    Uninstalling regex-2022.10.31:\n",
      "      Successfully uninstalled regex-2022.10.31\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2022.7.1\n",
      "    Uninstalling pytz-2022.7.1:\n",
      "      Successfully uninstalled pytz-2022.7.1\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.12.0\n",
      "    Uninstalling keras-2.12.0:\n",
      "      Successfully uninstalled keras-2.12.0\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 23.3.3\n",
      "    Uninstalling flatbuffers-23.3.3:\n",
      "      Successfully uninstalled flatbuffers-23.3.3\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2022.12.7\n",
      "    Uninstalling certifi-2022.12.7:\n",
      "      Successfully uninstalled certifi-2022.12.7\n",
      "  Attempting uninstall: zipp\n",
      "    Found existing installation: zipp 3.15.0\n",
      "    Uninstalling zipp-3.15.0:\n",
      "      Successfully uninstalled zipp-3.15.0\n",
      "  Attempting uninstall: Werkzeug\n",
      "    Found existing installation: Werkzeug 2.2.3\n",
      "    Uninstalling Werkzeug-2.2.3:\n",
      "      Successfully uninstalled Werkzeug-2.2.3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.15\n",
      "    Uninstalling urllib3-1.26.15:\n",
      "      Successfully uninstalled urllib3-1.26.15\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.0\n",
      "    Uninstalling tqdm-4.65.0:\n",
      "      Successfully uninstalled tqdm-4.65.0\n",
      "  Attempting uninstall: tomli\n",
      "    Found existing installation: tomli 2.0.1\n",
      "    Uninstalling tomli-2.0.1:\n",
      "      Successfully uninstalled tomli-2.0.1\n",
      "  Attempting uninstall: threadpoolctl\n",
      "    Found existing installation: threadpoolctl 3.1.0\n",
      "    Uninstalling threadpoolctl-3.1.0:\n",
      "      Successfully uninstalled threadpoolctl-3.1.0\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.0\n",
      "    Uninstalling tensorboard-data-server-0.7.0:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.0\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: rsa\n",
      "    Found existing installation: rsa 4.9\n",
      "    Uninstalling rsa-4.9:\n",
      "      Successfully uninstalled rsa-4.9\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.0.9\n",
      "    Uninstalling pyparsing-3.0.9:\n",
      "      Successfully uninstalled pyparsing-3.0.9\n",
      "  Attempting uninstall: Pygments\n",
      "    Found existing installation: Pygments 2.14.0\n",
      "    Uninstalling Pygments-2.14.0:\n",
      "      Successfully uninstalled Pygments-2.14.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: oauthlib\n",
      "    Found existing installation: oauthlib 3.2.2\n",
      "    Uninstalling oauthlib-3.2.2:\n",
      "      Successfully uninstalled oauthlib-3.2.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.4\n",
      "    Uninstalling numpy-1.22.4:\n",
      "      Successfully uninstalled numpy-1.22.4\n",
      "  Attempting uninstall: Markdown\n",
      "    Found existing installation: Markdown 3.4.3\n",
      "    Uninstalling Markdown-3.4.3:\n",
      "      Successfully uninstalled Markdown-3.4.3\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.2.0\n",
      "    Uninstalling joblib-1.2.0:\n",
      "      Successfully uninstalled joblib-1.2.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: docutils\n",
      "    Found existing installation: docutils 0.16\n",
      "    Uninstalling docutils-0.16:\n",
      "      Successfully uninstalled docutils-0.16\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.3\n",
      "    Uninstalling click-8.1.3:\n",
      "      Successfully uninstalled click-8.1.3\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 2.0.12\n",
      "    Uninstalling charset-normalizer-2.0.12:\n",
      "      Successfully uninstalled charset-normalizer-2.0.12\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 5.3.0\n",
      "    Uninstalling cachetools-5.3.0:\n",
      "      Successfully uninstalled cachetools-5.3.0\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.7.0\n",
      "    Uninstalling typer-0.7.0:\n",
      "      Successfully uninstalled typer-0.7.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.10.1\n",
      "    Uninstalling scipy-1.10.1:\n",
      "      Successfully uninstalled scipy-1.10.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.27.1\n",
      "    Uninstalling requests-2.27.1:\n",
      "      Successfully uninstalled requests-2.27.1\n",
      "  Attempting uninstall: pep517\n",
      "    Found existing installation: pep517 0.13.0\n",
      "    Uninstalling pep517-0.13.0:\n",
      "      Successfully uninstalled pep517-0.13.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.0\n",
      "    Uninstalling packaging-23.0:\n",
      "      Successfully uninstalled packaging-23.0\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.8.1\n",
      "    Uninstalling nltk-3.8.1:\n",
      "      Successfully uninstalled nltk-3.8.1\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 6.3.0\n",
      "    Uninstalling importlib-metadata-6.3.0:\n",
      "      Successfully uninstalled importlib-metadata-6.3.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.8.0\n",
      "    Uninstalling h5py-3.8.0:\n",
      "      Successfully uninstalled h5py-3.8.0\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.53.0\n",
      "    Uninstalling grpcio-1.53.0:\n",
      "      Successfully uninstalled grpcio-1.53.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.17.2\n",
      "    Uninstalling google-auth-2.17.2:\n",
      "      Successfully uninstalled google-auth-2.17.2\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 1.4.0\n",
      "    Uninstalling absl-py-1.4.0:\n",
      "      Successfully uninstalled absl-py-1.4.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "  Attempting uninstall: requests-oauthlib\n",
      "    Found existing installation: requests-oauthlib 1.3.1\n",
      "    Uninstalling requests-oauthlib-1.3.1:\n",
      "      Successfully uninstalled requests-oauthlib-1.3.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.5.3\n",
      "    Uninstalling pandas-1.5.3:\n",
      "      Successfully uninstalled pandas-1.5.3\n",
      "  Attempting uninstall: bleach\n",
      "    Found existing installation: bleach 6.0.0\n",
      "    Uninstalling bleach-6.0.0:\n",
      "      Successfully uninstalled bleach-6.0.0\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 1.0.0\n",
      "    Uninstalling google-auth-oauthlib-1.0.0:\n",
      "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.12.1\n",
      "    Uninstalling tensorboard-2.12.1:\n",
      "      Successfully uninstalled tensorboard-2.12.1\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.12.0\n",
      "    Uninstalling tensorflow-2.12.0:\n",
      "      Successfully uninstalled tensorflow-2.12.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
      "yfinance 0.2.17 requires pytz>=2022.5, but you have pytz 2021.3 which is incompatible.\n",
      "xarray 2022.12.0 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
      "xarray 2022.12.0 requires packaging>=21.3, but you have packaging 21.0 which is incompatible.\n",
      "xarray-einstats 0.5.1 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
      "tweepy 4.13.0 requires oauthlib<4,>=3.2.0, but you have oauthlib 3.1.1 which is incompatible.\n",
      "tweepy 4.13.0 requires requests<3,>=2.27.0, but you have requests 2.26.0 which is incompatible.\n",
      "tensorflow-hub 0.13.0 requires protobuf>=3.19.6, but you have protobuf 3.19.0 which is incompatible.\n",
      "statsmodels 0.13.5 requires packaging>=21.3, but you have packaging 21.0 which is incompatible.\n",
      "sqlalchemy 2.0.9 requires typing-extensions>=4.2.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n",
      "sphinx 3.5.4 requires docutils<0.17,>=0.12, but you have docutils 0.17.1 which is incompatible.\n",
      "rich 13.3.3 requires pygments<3.0.0,>=2.13.0, but you have pygments 2.10.0 which is incompatible.\n",
      "pydantic 1.10.7 requires typing-extensions>=4.2.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n",
      "plotnine 0.10.1 requires pandas>=1.3.5, but you have pandas 1.3.4 which is incompatible.\n",
      "optax 0.1.4 requires typing-extensions>=3.10.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n",
      "ml-dtypes 0.0.4 requires numpy>1.20, but you have numpy 1.19.5 which is incompatible.\n",
      "mizani 0.8.1 requires pandas>=1.3.5, but you have pandas 1.3.4 which is incompatible.\n",
      "matplotlib 3.7.1 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
      "librosa 0.10.0.post2 requires numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3, but you have numpy 1.19.5 which is incompatible.\n",
      "librosa 0.10.0.post2 requires typing-extensions>=4.1.1, but you have typing-extensions 3.7.4.3 which is incompatible.\n",
      "jaxlib 0.4.7+cuda11.cudnn86 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
      "jax 0.4.8 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
      "imbalanced-learn 0.10.1 requires joblib>=1.1.1, but you have joblib 1.1.0 which is incompatible.\n",
      "imbalanced-learn 0.10.1 requires scikit-learn>=1.0.2, but you have scikit-learn 1.0 which is incompatible.\n",
      "httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > \"3.0\", but you have pyparsing 3.0.1 which is incompatible.\n",
      "grpcio-status 1.48.2 requires grpcio>=1.48.2, but you have grpcio 1.41.0 which is incompatible.\n",
      "googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.0 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas~=1.5.3, but you have pandas 1.3.4 which is incompatible.\n",
      "google-colab 1.0.0 requires requests>=2.27.0, but you have requests 2.26.0 which is incompatible.\n",
      "google-cloud-translate 3.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.0 which is incompatible.\n",
      "google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.0 which is incompatible.\n",
      "google-cloud-firestore 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.0 which is incompatible.\n",
      "google-cloud-datastore 2.15.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.0 which is incompatible.\n",
      "google-cloud-bigquery 3.9.0 requires grpcio<2.0dev,>=1.47.0, but you have grpcio 1.41.0 which is incompatible.\n",
      "google-cloud-bigquery 3.9.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.0 which is incompatible.\n",
      "google-cloud-bigquery-storage 2.19.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.0 which is incompatible.\n",
      "google-api-core 2.11.0 requires google-auth<3.0dev,>=2.14.1, but you have google-auth 2.3.0 which is incompatible.\n",
      "google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.0 which is incompatible.\n",
      "flax 0.6.8 requires typing-extensions>=4.1.1, but you have typing-extensions 3.7.4.3 which is incompatible.\n",
      "flask 2.2.3 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
      "flask 2.2.3 requires Werkzeug>=2.2.2, but you have werkzeug 2.0.2 which is incompatible.\n",
      "cmdstanpy 1.1.0 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
      "chex 0.1.7 requires typing-extensions>=4.2.0; python_version < \"3.11\", but you have typing-extensions 3.7.4.3 which is incompatible.\n",
      "bokeh 2.4.3 requires typing-extensions>=3.10.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n",
      "astropy 5.2.2 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
      "arviz 0.15.1 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\n",
      "arviz 0.15.1 requires scipy>=1.8.0, but you have scipy 1.7.1 which is incompatible.\n",
      "arviz 0.15.1 requires typing-extensions>=4.1.0, but you have typing-extensions 3.7.4.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Automat-22.10.0 Keras-Preprocessing-1.1.2 Markdown-3.3.4 Pygments-2.10.0 SecretStorage-3.3.3 Werkzeug-2.0.2 absl-py-0.15.0 autopep8-1.6.0 backports.functools-lru-cache-1.6.4 bleach-4.1.0 build-0.7.0 cachetools-4.2.4 certifi-2021.10.8 charset-normalizer-2.0.7 clang-5.0 click-7.1.2 colorama-0.4.4 constantly-15.1.0 docutils-0.17.1 flatbuffers-1.12 google-auth-2.3.0 google-auth-oauthlib-0.4.6 grpcio-1.41.0 h5py-3.1.0 hindiwsd-1.1.2 hyperlink-21.0.0 idna-3.3 importlib-metadata-4.8.1 incremental-22.10.0 indic-transliteration-2.3.2 jeepney-0.8.0 joblib-1.1.0 keras-2.6.0 keyring-23.2.1 nltk-3.4.5 numpy-1.19.5 oauthlib-3.1.1 packaging-21.0 pandas-1.3.4 pbr-2.1.0 pep517-0.12.0 pkginfo-1.7.1 polyglot-tokenizer-2.0.2 protobuf-3.19.0 pycodestyle-2.8.0 pyiwn-0.0.5 pyparsing-3.0.1 python-crfsuite-0.9.7 pytz-2021.3 pywin32-ctypes-0.2.0 readme-renderer-30.0 regex-2020.10.11 requests-2.26.0 requests-oauthlib-1.3.0 requests-toolbelt-0.9.1 rfc3986-1.5.0 rsa-4.7.2 scikit-learn-1.0 scipy-1.7.1 selenium-3.141.0 seqeval-1.2.2 six-1.15.0 spello-1.2.0 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.6.0 tensorflow-estimator-2.6.0 termcolor-1.1.0 threadpoolctl-3.0.0 tomli-1.2.1 tqdm-4.62.3 twine-3.4.2 twisted-21.7.0 typer-0.3.2 typing-extensions-3.7.4.3 urllib3-1.26.7 wrapt-1.12.1 zipp-3.6.0 zope.interface-6.0\n"
     ]
    }
   ],
   "source": [
    "pip install hindiwsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5XH3ctNOVTIC",
    "outputId": "238b5d11-2055-43f4-e011-24a90f35934b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[██████████████████████████████████████████████████]\n"
     ]
    }
   ],
   "source": [
    "from hindiwsd import wsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O59t4h0Lcjzz"
   },
   "outputs": [],
   "source": [
    "tokens=' '.join(stemmed_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "P_9MaYtX583I",
    "outputId": "e0327c58-e472-4a16-e05a-776798e3255e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'गोव क यात्र बहुत अच्छ रह समुद्र तट बहुत गर्म थ मुझ समुद्र तट पर खेलन मे बहुत मज आय मेर बेट बहुत गुस् स मे थ जिंदग मे कई बार ऐस होत ह क जब हम अपन सोचन क तरीक बदलत ह तब हमार जिंदग भ बदल जात ह । जिंदग मे ज कुछ भ होत ह व सिर्फ ह होत ह बाक त हम ज हुआ ह उसक बार मे क्य सोचत ह यान क हमार reaction ह होत ह । एक लड़क थ ज देख नही सकत थ । य लड़क भीख मांगकर अपन जीवन गुजारत थ । एक दिन य अँध लड़क हर रोज क तरह एक बड़ स building क आग बैठकर भीख मांग रह थ । उस समय वह स एक अनमोल नामक लड़क गुजरत ह और इस अंध लड़क क भीख मांगत हुआ देखत ह । अनमोल उस building मे एक ऑफिस मे काम करत थ । जब अनमोल न देख क य लड़क भीख मांग रह ह त व उसक मदद करन क लिए उसक पास पहुँचत ह । अनमोल वह जाकर देखत ह क इस लड़क क पास एक डिब्ब ह और उस डिब्ब मे थोड़ सिक्क भ ह । कुछ लोग आत ह इस लड़क क पैस देत ह और कुछ लोग उस बस देखकर ह चल जात ह । अनमोल न य भ देख क इस लड़क क पीछ एक बोर्ड लग हुआ थ जिस पर लिख थ क मे एक अँध लड़क ह मेर मदद कीजिय । य सब देखकर अनमोल अब य सोचन लग क इस दुनिय मे भ बड़ कमाल क लोग ह जिस मदद क जरुरत ह उस कोई नही करत ह । उस बोर्ड पर ज लिख थ उस व मिट देत ह और उसक जगह पर और कुछ लिख देत ह । अब अनमोल कुछ पैस उस डिब्ब मे रखत ह और ऑफिस क और बढ़त ह । व कहत ह क भैय मेन आपक आपक आन क आहट स ह पहचान लिय । आप वह ह ज सुबह आय थ और मेर इस बोर्ड पर कुछ लिख कर गए थ । मे आपक सिर्फ इतन पूछन चाहत ह क आपन ऐस क्य लिख थ मेर बोर्ड पर क ज भ यहा स गुजरत ह व मुझ कुछ न कुछ देकर ह जात ह । अनमोल न कह क मेन ज तुम्हार बोर्ड पर लिख हुआ थ उस मिट दिय । तुम्हार बोर्ड पर लिख हुआ थ क मे एक अँध लड़क ह मेर मदद कीजिय मेन उसक मिट दिय और य लिख दिय क आज क दिन बहुत खूबसूरत ह लेकिन मे देख नही सकत ह'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FYXwzvaaUAhK",
    "outputId": "f175b061-7bc2-48eb-a714-17a672ec0190"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [], [], [], [], [], [('आ', 'VERB')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('आ', 'VERB')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('आ', 'VERB')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('इ', 'ADJECTIVE')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('आ', 'VERB')], [], [], [], [], [], [], [], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('इ', 'ADJECTIVE')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('आ', 'VERB')], [], [], [], [], [('इ', 'ADJECTIVE')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [], [], [], [], [('इ', 'ADJECTIVE')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('आ', 'VERB')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [('इ', 'ADJECTIVE')], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('न', 'NOUN')], [], [('आ', 'VERB')], [], [], [], [('आ', 'VERB')], [], [], [], [('आ', 'VERB')], [('न', 'NOUN')], [], [], [], [('आ', 'VERB')], [], [], [], [], [], [], [], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [('आ', 'VERB')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('आ', 'VERB')], [], [], [], [], [], [], [], [], [], [], [], [('इ', 'ADJECTIVE')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('आ', 'VERB')], [], [], [], [], [], [], [], [], [], [('इ', 'ADJECTIVE')], [], [('न', 'NOUN')], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [], [], [], [('आ', 'VERB')], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('आ', 'VERB')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('आ', 'VERB')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('आ', 'VERB')], [], [], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], [], [('न', 'NOUN')], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "postag=[]\n",
    "for i in range(len(tokens)):\n",
    "  postag.append(wsd.POS_tagger(tokens[i]))\n",
    "\n",
    "print(postag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7TGTHBYgOYw"
   },
   "source": [
    "SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rXoC7RHdgk81",
    "outputId": "a8161cd6-dea8-4fdd-856b-8be665f74522"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting deep_translator\n",
      "  Downloading deep_translator-1.10.1-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.9/dist-packages (from deep_translator) (2.27.1)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.9/dist-packages (from deep_translator) (4.11.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2022.12.7)\n",
      "Installing collected packages: deep_translator\n",
      "Successfully installed deep_translator-1.10.1\n"
     ]
    }
   ],
   "source": [
    "pip install deep_translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xaprFM94gwWd",
    "outputId": "83d870d4-1020-4792-c7ad-645823362dab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from vaderSentiment) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->vaderSentiment) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->vaderSentiment) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->vaderSentiment) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->vaderSentiment) (1.26.15)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n"
     ]
    }
   ],
   "source": [
    "pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CUUWYnJwgaue"
   },
   "outputs": [],
   "source": [
    "# codecs provides access to the internal Python codec registry\n",
    "import codecs\n",
    "\n",
    "# This is to translate the text from Hindi to English\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# This is to analyse the sentiment of text\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lfM8vkrOgeXz",
    "outputId": "6d0834d5-4f98-493e-fcd0-1a70779f9ac3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['गोवा की यात्रा बहुत अच्छी रही।\\n', 'समुद्र तट बहुत गर्म थे।\\n', 'मुझे समुद्र , तट पर खेलने में बहुत मजा आया।\\n', 'मेरी बेटी बहुत गुस् 2 से में थी।\\n', 'जिंदगी में कई बार ऐसा होता है की जब हम अपने सोचने का तरीका बदलते है तब हमारी जिंदगी भी बदल जाती है । जिंदगी में जो कुछ भी होता है वो सिर्फ 1 % ही होता है बाकि 99 % तो हम जो हुआ है उसके बारे में क्या सोचते है यानि की हमारा Reaction ही होता है ।\\n', '\\n', 'एक लड़का था जो देख नहीं सकता था । ये लड़का भीख मांगकर अपना जीवन गुजारता था । एक दिन ये अँधा लड़का हर रोज की तरह एक बड़ी सी Building के आगे बैठकर भीख मांग रहा था । उसी समय वहा से एक अनमोल नामका लड़का गुजरता है और इस अंधे लड़के को भीख मांगता हुआ देखता है । अनमोल उसी Building में एक ऑफिस में काम करता था । जब अनमोल ने देखा की ये लड़का भीख मांग रहा है तो वो उसकी मदद करने के लिए उसके पास पहुँचता है । अनमोल वहा जाकर देखता है की इस लड़के के पास एक डिब्बा है और उस डिब्बे में थोड़े सिक्के भी है । कुछ लोग आते है इस लड़के को पैसे देते है और कुछ लोग उसे बस देखकर ही चले जाते है ।\\n', '\\n', 'अनमोल ने ये भी देखा की इस लड़के के पीछे एक बोर्ड लगा हुआ था जिस पर लिखा था की में एक अँधा लड़का हु , मेरी मदद कीजिये । ये सब देखकर अनमोल अब ये सोचने लगा की इस दुनिया में भी बड़े कमाल के लोग है जिसे मदद की जरुरत है उसे कोई नहीं करता है ।\\n', '\\n', 'उस बोर्ड पर जो लिखा था उसे वो मिटा देता है और उसकी जगह पर और कुछ लिख देता है । अब अनमोल कुछ पैसे उस डिब्बे में रखता है और ऑफिस की और बढ़ता है । वो कहता है की भैया मेने आपको आपकी आने की आहट से ही पहचान लिया । आप वही है जो सुबह आये थे और मेरे इस बोर्ड पर कुछ लिख कर गए थे । में आपको सिर्फ इतना पूछना चाहता हु की आपने ऐसा क्या लिखा था मेरे बोर्ड पर की जो भी यहाँ से गुजरता है वो मुझे कुछ ना कुछ देकर ही जाता है ।\\n', '\\n', 'अनमोल ने कहा की मेने जो तुम्हारे बोर्ड पर लिखा हुआ था उसे मिटा दिया । तुम्हारे बोर्ड पर लिखा हुआ था की में एक अँधा लड़का हु , मेरी मदद कीजिये मेने उसको मिटा दिया और ये लिख दिया की आज का दिन बहुत खूबसूरत है , लेकिन में देख नहीं सकता हु !']\n"
     ]
    }
   ],
   "source": [
    "# Read the hindi text into 'sentences'\n",
    "with codecs.open('SampleHindiText.txt', encoding='utf-8') as f:\n",
    "\tsentences = f.readlines()\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KGnyBVjPgeal",
    "outputId": "00c6ecd2-30bc-4714-cb3e-9ba11fa37fe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translated Sentence= Goa trip was great. \n",
      "Dictionary= {'neg': 0.0, 'neu': 0.423, 'pos': 0.577, 'compound': 0.6249}\n",
      "It is a Positive Sentence\n",
      "\n",
      "Translated Sentence= The beaches were very hot. \n",
      "Dictionary= {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "It is a Neutral Sentence\n",
      "\n",
      "Translated Sentence= I enjoyed playing in the sea, on the beach. \n",
      "Dictionary= {'neg': 0.0, 'neu': 0.579, 'pos': 0.421, 'compound': 0.6249}\n",
      "It is a Positive Sentence\n",
      "\n",
      "Translated Sentence= My daughter was very angry. \n",
      "Dictionary= {'neg': 0.473, 'neu': 0.527, 'pos': 0.0, 'compound': -0.5563}\n",
      "It is a Negative Sentence\n",
      "\n",
      "Translated Sentence= Many times it happens in life that when we change our way of thinking then our life also changes. Whatever happens in life is only 1%, the rest 99% is what we think about what has happened, that is, our reaction is the same. \n",
      "Dictionary= {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "It is a Neutral Sentence\n",
      "\n",
      "Translated Sentence=  \n",
      "Dictionary= {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
      "It is a Neutral Sentence\n",
      "\n",
      "Translated Sentence= There was a boy who could not see. This boy used to spend his life by begging. One day this blind boy was begging sitting in front of a big building like everyday. At the same time a boy named Anmol passes by and sees this blind boy begging. Anmol used to work in an office in the same building. When Anmol sees that this boy is begging, he reaches out to him to help him. Anmol goes there and sees that this boy has a box and there are some coins in that box. Some people come and give money to this boy and some people go away just seeing him. \n",
      "Dictionary= {'neg': 0.052, 'neu': 0.894, 'pos': 0.053, 'compound': -0.2144}\n",
      "It is a Negative Sentence\n",
      "\n",
      "Translated Sentence=  \n",
      "Dictionary= {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
      "It is a Neutral Sentence\n",
      "\n",
      "Translated Sentence= Anmol also saw that there was a board behind this boy on which it was written that I am a blind boy, help me. Seeing all this, Anmol now started thinking that there are wonderful people in this world too, no one helps those who need help. \n",
      "Dictionary= {'neg': 0.124, 'neu': 0.717, 'pos': 0.159, 'compound': 0.4617}\n",
      "It is a Positive Sentence\n",
      "\n",
      "Translated Sentence=  \n",
      "Dictionary= {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
      "It is a Neutral Sentence\n",
      "\n",
      "Translated Sentence= He erases what was written on that board and writes something else in its place. Now Anmol puts some money in that box and moves towards the office. He says that brother, I recognized you only by the sound of your arrival. You are the one who came in the morning and left after writing something on this board of mine. I just want to ask you that what did you write on my board that whoever passes by here leaves after giving me something or the other. \n",
      "Dictionary= {'neg': 0.0, 'neu': 0.959, 'pos': 0.041, 'compound': 0.4019}\n",
      "It is a Positive Sentence\n",
      "\n",
      "Translated Sentence=  \n",
      "Dictionary= {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
      "It is a Neutral Sentence\n",
      "\n",
      "Translated Sentence= Anmol said that I erased what was written on your board. It was written on your board that I am a blind boy, help me, I erased it and wrote that today is a very beautiful day, but I cannot see! \n",
      "Dictionary= {'neg': 0.041, 'neu': 0.852, 'pos': 0.106, 'compound': 0.4383}\n",
      "It is a Positive Sentence\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "\ttranslated_text = GoogleTranslator(source='auto', target='en').translate(sentence)\n",
    "\t#print(translated_text)\n",
    "\tanalyzer = SentimentIntensityAnalyzer()\n",
    "\tsentiment_dict = analyzer.polarity_scores(translated_text)\n",
    "\t\n",
    "\tprint(\"\\nTranslated Sentence=\",translated_text, \"\\nDictionary=\",sentiment_dict)\n",
    "\tif sentiment_dict['compound'] >= 0.05 :\n",
    "\t\t\tprint(\"It is a Positive Sentence\")\n",
    "\t\t\t\n",
    "\telif sentiment_dict['compound'] <= - 0.05 :\n",
    "\t\t\tprint(\"It is a Negative Sentence\")\t\n",
    "\telse :\n",
    "\t\tprint(\"It is a Neutral Sentence\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0qWe0cxgeeB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
